{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['keyword', 'location', 'text']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>army</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vote for #Directioners vs #Queens in the 5th r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>cliff%20fall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#FunnyNews #Business Watch the moment a cliff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>weapon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Back to back like I'm on the cover of lethal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crackdown 3 Destruction Restricted to Multipla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6977</th>\n",
       "      <td>twister</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's alil twister at Tha end to! I was like oh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>collided</td>\n",
       "      <td>Johannesburg, South Africa</td>\n",
       "      <td>2 pple have been confirmed dead and over 20 re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>drought</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>'It's an eerie way of revealing both our histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>bombing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan Marks 70th Anniversary of Hiroshima Atom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>hijacker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medieval airplane hijacker testa: earnings the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>evacuation</td>\n",
       "      <td>Bend, Oregon</td>\n",
       "      <td>Update: Bend FD says roofing co. workers accid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5709 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword                    location  \\\n",
       "355           army                         NaN   \n",
       "1570  cliff%20fall                         NaN   \n",
       "7198        weapon                         NaN   \n",
       "2614   destruction                         NaN   \n",
       "6977       twister                         NaN   \n",
       "...            ...                         ...   \n",
       "1726      collided  Johannesburg, South Africa   \n",
       "2885       drought             Los Angeles, CA   \n",
       "1144       bombing                         NaN   \n",
       "4371      hijacker                         NaN   \n",
       "3370    evacuation                Bend, Oregon   \n",
       "\n",
       "                                                   text  \n",
       "355   Vote for #Directioners vs #Queens in the 5th r...  \n",
       "1570  #FunnyNews #Business Watch the moment a cliff ...  \n",
       "7198  Back to back like I'm on the cover of lethal w...  \n",
       "2614  Crackdown 3 Destruction Restricted to Multipla...  \n",
       "6977  It's alil twister at Tha end to! I was like oh...  \n",
       "...                                                 ...  \n",
       "1726  2 pple have been confirmed dead and over 20 re...  \n",
       "2885  'It's an eerie way of revealing both our histo...  \n",
       "1144  Japan Marks 70th Anniversary of Hiroshima Atom...  \n",
       "4371  Medieval airplane hijacker testa: earnings the...  \n",
       "3370  Update: Bend FD says roofing co. workers accid...  \n",
       "\n",
       "[5709 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords & Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of stopwords & punctuation\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet_text):\n",
    "    \n",
    "    #Remove url links from text\n",
    "    tweet_text = re.sub(r\"http\\S+\", \"\", tweet_text)\n",
    "    \n",
    "    #Tokenize text using NLTK function\n",
    "    tokens = nltk.word_tokenize(tweet_text)\n",
    "    \n",
    "    #Make all words lowercase and remove words in stopwords_list\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    \n",
    "    return stopwords_removed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = list(map(process_tweet, X_train['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total unique words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15132"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use a set so that no duplicate words are counted\n",
    "total_vocab = set()\n",
    "for text in X_train_processed:\n",
    "    total_vocab.update(text)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 605),\n",
       " (\"n't\", 342),\n",
       " ('like', 263),\n",
       " ('amp', 259),\n",
       " (\"'m\", 184),\n",
       " ('fire', 181),\n",
       " ('get', 166),\n",
       " ('via', 163),\n",
       " ('new', 160),\n",
       " ('news', 153),\n",
       " ('people', 144),\n",
       " ('one', 140),\n",
       " ('video', 130),\n",
       " ('disaster', 119),\n",
       " ('2', 118),\n",
       " ('emergency', 115),\n",
       " ('would', 106),\n",
       " ('police', 103),\n",
       " (\"'re\", 101),\n",
       " ('still', 95),\n",
       " ('man', 93),\n",
       " ('body', 92),\n",
       " ('back', 91),\n",
       " ('..', 91),\n",
       " ('going', 91),\n",
       " ('crash', 91),\n",
       " ('got', 90),\n",
       " ('storm', 89),\n",
       " ('day', 88),\n",
       " ('us', 88),\n",
       " ('california', 84),\n",
       " ('burning', 84),\n",
       " ('know', 81),\n",
       " ('suicide', 79),\n",
       " ('time', 79),\n",
       " ('two', 78),\n",
       " ('today', 78),\n",
       " ('buildings', 78),\n",
       " ('ca', 78),\n",
       " ('youtube', 78),\n",
       " ('see', 77),\n",
       " ('love', 76),\n",
       " ('first', 76),\n",
       " ('world', 75),\n",
       " ('killed', 75),\n",
       " ('families', 75),\n",
       " ('fires', 74),\n",
       " ('rt', 74),\n",
       " ('nuclear', 74),\n",
       " ('attack', 74)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_concat = []\n",
    "for article in X_train_processed:\n",
    "    articles_concat += article\n",
    "\n",
    "articles_freqdist = FreqDist(articles_concat)\n",
    "articles_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer() function takes in whole blocks of text, not individual words.  Therefore, we join the lists stored in X_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processed = []\n",
    "for text in X_train_processed:\n",
    "    text_string = ' '.join(text)\n",
    "    text_processed.append(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5709"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct number of text entries.  Now join with dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vote directioners vs queens 5th round billboard fanarmyfaceoff'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processed = pd.Series(text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesfay/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_train['text_processed'] = text_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>355</td>\n",
       "      <td>army</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vote for #Directioners vs #Queens in the 5th r...</td>\n",
       "      <td>vote directioners vs queens 5th round billboar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1570</td>\n",
       "      <td>cliff%20fall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#FunnyNews #Business Watch the moment a cliff ...</td>\n",
       "      <td>funnynews business watch moment cliff collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7198</td>\n",
       "      <td>weapon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Back to back like I'm on the cover of lethal w...</td>\n",
       "      <td>back back like 'm cover lethal weapon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2614</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crackdown 3 Destruction Restricted to Multipla...</td>\n",
       "      <td>crackdown 3 destruction restricted multiplayer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6977</td>\n",
       "      <td>twister</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's alil twister at Tha end to! I was like oh...</td>\n",
       "      <td>'s alil twister tha end like oh nah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5704</th>\n",
       "      <td>1726</td>\n",
       "      <td>collided</td>\n",
       "      <td>Johannesburg, South Africa</td>\n",
       "      <td>2 pple have been confirmed dead and over 20 re...</td>\n",
       "      <td>2 pple confirmed dead 20 rescued many went mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5705</th>\n",
       "      <td>2885</td>\n",
       "      <td>drought</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>'It's an eerie way of revealing both our histo...</td>\n",
       "      <td>'it 's eerie way revealing history possible fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5706</th>\n",
       "      <td>1144</td>\n",
       "      <td>bombing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan Marks 70th Anniversary of Hiroshima Atom...</td>\n",
       "      <td>japan marks 70th anniversary hiroshima atomic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707</th>\n",
       "      <td>4371</td>\n",
       "      <td>hijacker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medieval airplane hijacker testa: earnings the...</td>\n",
       "      <td>medieval airplane hijacker testa earnings dist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708</th>\n",
       "      <td>3370</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>Bend, Oregon</td>\n",
       "      <td>Update: Bend FD says roofing co. workers accid...</td>\n",
       "      <td>update bend fd says roofing co. workers accide...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5709 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index       keyword                    location  \\\n",
       "0       355          army                         NaN   \n",
       "1      1570  cliff%20fall                         NaN   \n",
       "2      7198        weapon                         NaN   \n",
       "3      2614   destruction                         NaN   \n",
       "4      6977       twister                         NaN   \n",
       "...     ...           ...                         ...   \n",
       "5704   1726      collided  Johannesburg, South Africa   \n",
       "5705   2885       drought             Los Angeles, CA   \n",
       "5706   1144       bombing                         NaN   \n",
       "5707   4371      hijacker                         NaN   \n",
       "5708   3370    evacuation                Bend, Oregon   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Vote for #Directioners vs #Queens in the 5th r...   \n",
       "1     #FunnyNews #Business Watch the moment a cliff ...   \n",
       "2     Back to back like I'm on the cover of lethal w...   \n",
       "3     Crackdown 3 Destruction Restricted to Multipla...   \n",
       "4     It's alil twister at Tha end to! I was like oh...   \n",
       "...                                                 ...   \n",
       "5704  2 pple have been confirmed dead and over 20 re...   \n",
       "5705  'It's an eerie way of revealing both our histo...   \n",
       "5706  Japan Marks 70th Anniversary of Hiroshima Atom...   \n",
       "5707  Medieval airplane hijacker testa: earnings the...   \n",
       "5708  Update: Bend FD says roofing co. workers accid...   \n",
       "\n",
       "                                         text_processed  \n",
       "0     vote directioners vs queens 5th round billboar...  \n",
       "1     funnynews business watch moment cliff collapse...  \n",
       "2                 back back like 'm cover lethal weapon  \n",
       "3     crackdown 3 destruction restricted multiplayer...  \n",
       "4                   's alil twister tha end like oh nah  \n",
       "...                                                 ...  \n",
       "5704  2 pple confirmed dead 20 rescued many went mis...  \n",
       "5705  'it 's eerie way revealing history possible fa...  \n",
       "5706  japan marks 70th anniversary hiroshima atomic ...  \n",
       "5707  medieval airplane hijacker testa earnings dist...  \n",
       "5708  update bend fd says roofing co. workers accide...  \n",
       "\n",
       "[5709 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True)\n",
    "\n",
    "tf_idf_data_train = vectorizer.fit_transform(X_train['text_processed'])\n",
    "\n",
    "# tf_idf_data_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 14146)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data contains 5709 articles with 14k unique words in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create pipeline\n",
    "pipe_forest = Pipeline([('forest', RandomForestClassifier(random_state=70, n_jobs=-1, bootstrap=True))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid parameter\n",
    "grid_forest = [{'forest__n_estimators': [100, 200, 300],\n",
    "             'forest__max_depth': [1, 5, 15, 25, 50],\n",
    "             'forest__min_samples_split': [2, 5, 10, 25, 50], \n",
    "             'forest__min_samples_leaf': [1, 3, 5, 10, 25], \n",
    "             'forest__criterion': ['gini', 'entropy'],\n",
    "             'forest__max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'forest__max_samples': [None, .2, .5, .8]\n",
    "             }]\n",
    "\n",
    "# Create the grid, with \"pipe\" as the estimator\n",
    "gridsearch_forest = RandomizedSearchCV(estimator=pipe_forest, \n",
    "                          param_distributions=grid_forest, \n",
    "#                           scoring=['r2', 'neg_root_mean_squared_error'], #Include RMSE in Results\n",
    "#                           refit='r2', #Choose best model based on R^2\n",
    "                          return_train_score=True, #Include training results in cv_results\n",
    "                          cv=5, #Use 5 folds in CV process\n",
    "                          n_iter=20, #Try 20 hyperparameter combinations\n",
    "                          n_jobs=-1, #Use paralell computing\n",
    "                          verbose=8) #Give updates on progress during fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   10.8s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done  98 out of 100 | elapsed:   32.9s remaining:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   32.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('forest',\n",
       "                                              RandomForestClassifier(n_jobs=-1,\n",
       "                                                                     random_state=70))]),\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions=[{'forest__criterion': ['gini',\n",
       "                                                               'entropy'],\n",
       "                                         'forest__max_depth': [1, 5, 15, 25,\n",
       "                                                               50],\n",
       "                                         'forest__max_features': ['auto',\n",
       "                                                                  'sqrt',\n",
       "                                                                  'log2'],\n",
       "                                         'forest__max_samples': [None, 0.2, 0.5,\n",
       "                                                                 0.8],\n",
       "                                         'forest__min_samples_leaf': [1, 3, 5,\n",
       "                                                                      10, 25],\n",
       "                                         'forest__min_samples_split': [2, 5, 10,\n",
       "                                                                       25, 50],\n",
       "                                         'forest__n_estimators': [100, 200,\n",
       "                                                                  300]}],\n",
       "                   return_train_score=True, verbose=8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_forest.fit(tf_idf_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forest__n_estimators': 300,\n",
       " 'forest__min_samples_split': 2,\n",
       " 'forest__min_samples_leaf': 3,\n",
       " 'forest__max_samples': None,\n",
       " 'forest__max_features': 'auto',\n",
       " 'forest__max_depth': 25,\n",
       " 'forest__criterion': 'entropy'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.698194351284936"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_forest.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_forest__n_estimators</th>\n",
       "      <th>param_forest__min_samples_split</th>\n",
       "      <th>param_forest__min_samples_leaf</th>\n",
       "      <th>param_forest__max_samples</th>\n",
       "      <th>param_forest__max_features</th>\n",
       "      <th>param_forest__max_depth</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.282258</td>\n",
       "      <td>0.411456</td>\n",
       "      <td>0.205505</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>auto</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698194</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716006</td>\n",
       "      <td>0.706372</td>\n",
       "      <td>0.714473</td>\n",
       "      <td>0.718853</td>\n",
       "      <td>0.717163</td>\n",
       "      <td>0.714573</td>\n",
       "      <td>0.004344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.437897</td>\n",
       "      <td>0.604151</td>\n",
       "      <td>0.206177</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>300</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>auto</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.644243</td>\n",
       "      <td>0.007206</td>\n",
       "      <td>4</td>\n",
       "      <td>0.648128</td>\n",
       "      <td>0.650974</td>\n",
       "      <td>0.652945</td>\n",
       "      <td>0.654259</td>\n",
       "      <td>0.654335</td>\n",
       "      <td>0.652128</td>\n",
       "      <td>0.002341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.272860</td>\n",
       "      <td>0.047754</td>\n",
       "      <td>0.207166</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>300</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595375</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>5</td>\n",
       "      <td>0.598642</td>\n",
       "      <td>0.600175</td>\n",
       "      <td>0.597110</td>\n",
       "      <td>0.597986</td>\n",
       "      <td>0.606392</td>\n",
       "      <td>0.600061</td>\n",
       "      <td>0.003320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.944793</td>\n",
       "      <td>0.135482</td>\n",
       "      <td>0.106173</td>\n",
       "      <td>0.001623</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666490</td>\n",
       "      <td>0.005564</td>\n",
       "      <td>3</td>\n",
       "      <td>0.675936</td>\n",
       "      <td>0.672214</td>\n",
       "      <td>0.677250</td>\n",
       "      <td>0.680753</td>\n",
       "      <td>0.679947</td>\n",
       "      <td>0.677220</td>\n",
       "      <td>0.003054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.937509</td>\n",
       "      <td>0.115371</td>\n",
       "      <td>0.104944</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>auto</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698020</td>\n",
       "      <td>0.006033</td>\n",
       "      <td>2</td>\n",
       "      <td>0.722575</td>\n",
       "      <td>0.716444</td>\n",
       "      <td>0.714473</td>\n",
       "      <td>0.723451</td>\n",
       "      <td>0.721541</td>\n",
       "      <td>0.719697</td>\n",
       "      <td>0.003568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "3        5.282258      0.411456         0.205505        0.001564   \n",
       "7        3.437897      0.604151         0.206177        0.001984   \n",
       "8        3.272860      0.047754         0.207166        0.001388   \n",
       "11       2.944793      0.135482         0.106173        0.001623   \n",
       "14       1.937509      0.115371         0.104944        0.001682   \n",
       "\n",
       "   param_forest__n_estimators param_forest__min_samples_split  \\\n",
       "3                         300                               2   \n",
       "7                         300                              25   \n",
       "8                         300                              10   \n",
       "11                        200                               5   \n",
       "14                        100                              50   \n",
       "\n",
       "   param_forest__min_samples_leaf param_forest__max_samples  \\\n",
       "3                               3                      None   \n",
       "7                              10                       0.8   \n",
       "8                               3                       0.8   \n",
       "11                              1                       0.8   \n",
       "14                              3                      None   \n",
       "\n",
       "   param_forest__max_features param_forest__max_depth  ... mean_test_score  \\\n",
       "3                        auto                      25  ...        0.698194   \n",
       "7                        auto                      15  ...        0.644243   \n",
       "8                        sqrt                       5  ...        0.595375   \n",
       "11                       sqrt                      15  ...        0.666490   \n",
       "14                       auto                      25  ...        0.698020   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "3        0.007061                1            0.716006            0.706372   \n",
       "7        0.007206                4            0.648128            0.650974   \n",
       "8        0.003355                5            0.598642            0.600175   \n",
       "11       0.005564                3            0.675936            0.672214   \n",
       "14       0.006033                2            0.722575            0.716444   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "3             0.714473            0.718853            0.717163   \n",
       "7             0.652945            0.654259            0.654335   \n",
       "8             0.597110            0.597986            0.606392   \n",
       "11            0.677250            0.680753            0.679947   \n",
       "14            0.714473            0.723451            0.721541   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "3           0.714573         0.004344  \n",
       "7           0.652128         0.002341  \n",
       "8           0.600061         0.003320  \n",
       "11          0.677220         0.003054  \n",
       "14          0.719697         0.003568  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_forest_df = pd.DataFrame.from_dict(gridsearch_forest.cv_results_)\n",
    "best_models = gridsearch_forest_df.loc[gridsearch_forest_df['rank_test_score'] < 6]\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
