{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train and test groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['keyword', 'location', 'text']]\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>army</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vote for #Directioners vs #Queens in the 5th r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>cliff%20fall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#FunnyNews #Business Watch the moment a cliff ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7198</th>\n",
       "      <td>weapon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Back to back like I'm on the cover of lethal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2614</th>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Crackdown 3 Destruction Restricted to Multipla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6977</th>\n",
       "      <td>twister</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It's alil twister at Tha end to! I was like oh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>collided</td>\n",
       "      <td>Johannesburg, South Africa</td>\n",
       "      <td>2 pple have been confirmed dead and over 20 re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>drought</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>'It's an eerie way of revealing both our histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>bombing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Japan Marks 70th Anniversary of Hiroshima Atom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>hijacker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Medieval airplane hijacker testa: earnings the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3370</th>\n",
       "      <td>evacuation</td>\n",
       "      <td>Bend, Oregon</td>\n",
       "      <td>Update: Bend FD says roofing co. workers accid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5709 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword                    location  \\\n",
       "355           army                         NaN   \n",
       "1570  cliff%20fall                         NaN   \n",
       "7198        weapon                         NaN   \n",
       "2614   destruction                         NaN   \n",
       "6977       twister                         NaN   \n",
       "...            ...                         ...   \n",
       "1726      collided  Johannesburg, South Africa   \n",
       "2885       drought             Los Angeles, CA   \n",
       "1144       bombing                         NaN   \n",
       "4371      hijacker                         NaN   \n",
       "3370    evacuation                Bend, Oregon   \n",
       "\n",
       "                                                   text  \n",
       "355   Vote for #Directioners vs #Queens in the 5th r...  \n",
       "1570  #FunnyNews #Business Watch the moment a cliff ...  \n",
       "7198  Back to back like I'm on the cover of lethal w...  \n",
       "2614  Crackdown 3 Destruction Restricted to Multipla...  \n",
       "6977  It's alil twister at Tha end to! I was like oh...  \n",
       "...                                                 ...  \n",
       "1726  2 pple have been confirmed dead and over 20 re...  \n",
       "2885  'It's an eerie way of revealing both our histo...  \n",
       "1144  Japan Marks 70th Anniversary of Hiroshima Atom...  \n",
       "4371  Medieval airplane hijacker testa: earnings the...  \n",
       "3370  Update: Bend FD says roofing co. workers accid...  \n",
       "\n",
       "[5709 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords & Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create List of stopwords & punctuation\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet_text):\n",
    "    \n",
    "    #Remove url links from text\n",
    "    tweet_text = re.sub(r\"http\\S+\", \"\", tweet_text)\n",
    "    \n",
    "    #Tokenize text using NLTK function\n",
    "    tokens = nltk.word_tokenize(tweet_text)\n",
    "    \n",
    "    #Make all words lowercase and remove words in stopwords_list\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    \n",
    "    return stopwords_removed        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = list(map(process_tweet, X_train['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find total unique words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15132"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use a set so that no duplicate words are counted\n",
    "total_vocab = set()\n",
    "for text in X_train_processed:\n",
    "    total_vocab.update(text)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'s\", 605),\n",
       " (\"n't\", 342),\n",
       " ('like', 263),\n",
       " ('amp', 259),\n",
       " (\"'m\", 184),\n",
       " ('fire', 181),\n",
       " ('get', 166),\n",
       " ('via', 163),\n",
       " ('new', 160),\n",
       " ('news', 153),\n",
       " ('people', 144),\n",
       " ('one', 140),\n",
       " ('video', 130),\n",
       " ('disaster', 119),\n",
       " ('2', 118),\n",
       " ('emergency', 115),\n",
       " ('would', 106),\n",
       " ('police', 103),\n",
       " (\"'re\", 101),\n",
       " ('still', 95),\n",
       " ('man', 93),\n",
       " ('body', 92),\n",
       " ('back', 91),\n",
       " ('..', 91),\n",
       " ('going', 91),\n",
       " ('crash', 91),\n",
       " ('got', 90),\n",
       " ('storm', 89),\n",
       " ('day', 88),\n",
       " ('us', 88),\n",
       " ('california', 84),\n",
       " ('burning', 84),\n",
       " ('know', 81),\n",
       " ('suicide', 79),\n",
       " ('time', 79),\n",
       " ('two', 78),\n",
       " ('today', 78),\n",
       " ('buildings', 78),\n",
       " ('ca', 78),\n",
       " ('youtube', 78),\n",
       " ('see', 77),\n",
       " ('love', 76),\n",
       " ('first', 76),\n",
       " ('world', 75),\n",
       " ('killed', 75),\n",
       " ('families', 75),\n",
       " ('fires', 74),\n",
       " ('rt', 74),\n",
       " ('nuclear', 74),\n",
       " ('attack', 74)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_concat = []\n",
    "for article in X_train_processed:\n",
    "    articles_concat += article\n",
    "\n",
    "articles_freqdist = FreqDist(articles_concat)\n",
    "articles_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer() function takes in whole blocks of text, not individual words.  Therefore, we remove stopwords from the original text stored in our X_train dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet_text):\n",
    "    #Remove url links from text\n",
    "    tweet_text = re.sub(r\"http\\S+\", \"\", tweet_text)\n",
    "    \n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    #Create List of stopwords & punctuation\n",
    "    stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "    stopwords_list += [\"''\", '\"\"', '...', '``']\n",
    "    \n",
    "    for word in stopwords_list:\n",
    "        text = text.replace(word, '')\n",
    "#     stopwords_removed = [token.lower() for token in tokens if token.lower() not in stopwords_list]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fbbc96513c79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unicode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf_idf_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtf_idf_data_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsgroups_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(strip_accents='unicode', lowercase=True)\n",
    "\n",
    "tf_idf_data_train = vectorizer.fit_transform(data)\n",
    "\n",
    "tf_idf_data_test = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
